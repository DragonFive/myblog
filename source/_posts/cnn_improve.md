---
title: 神经网络的压缩优化

date: 2017/8/5 12:04:12

categories:
- 深度学习
tags:
- deeplearning
- 网络优化
- 神经网络
---
[TOC]

回顾一下几个经典模型，我们主要看看深度和caffe模型大小，[神经网络模型演化](https://dragonfive.github.io/2017-07-05/deep_learning_model/)

![各种CNN模型][1]

模型大小(参数量)和模型的深浅并非是正相关。

<!--more-->

# 一些经典的模型设计路线

## fully connect to local connect 全连接到卷积神经网络  1x1卷积
Alexnet[1]是一个8层的卷积神经网络，有约**60M个参数**，如果采用**32bit float存下来有200M**。值得一提的是，AlexNet中仍然有3个全连接层，其参数量占比参数总量超过了90%。

下面举一个例子，假如输入为28×28×192，输出feature map通道数为128。那么，直接接3×3卷积，参数量为3×3×192×128=221184。

如果先用**1×1卷积进行降维到96个通道**，然后再用3×3升维到128，则参数量为：1×1×192×96+3×3×96×128=129024，参数量减少一半。虽然参数量减少不是很明显，但是如果1×1输出维度降低到48呢？则参数量又减少一半。对于上千层的大网络来说，效果还是很明显了。

移动端对模型大小很敏感。下载一个100M的app与50M的app，首先用户心理接受程度就不一样。

原则上降低通道数是会降低性能的，这里为什么却可以降维呢？我们可以从很多embedding技术，比如PCA等中得到思考，**降低一定的维度可以去除冗余数据**，损失的精度其实很多情况下都不会对我们解决问题有很大影响。

1×1卷积，在 GoogLeNet Inception v1以及后续版本，ResNet中都大量得到应用，有减少模型参数的作用。


# reference

[知乎:为了压榨CNN模型，这几年大家都干了什么](https://zhuanlan.zhihu.com/p/25797790)


  [1]: https://www.github.com/DragonFive/CVBasicOp/raw/master/1502693251377.jpg